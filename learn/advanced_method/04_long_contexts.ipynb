{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kalin\\AppData\\Local\\Temp\\ipykernel_45592\\2887570783.py:23: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding = OpenAIEmbeddings(base_url=\"...\")\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "\n",
        "# Load pdf\n",
        "loader = PyPDFLoader(\"..\\\\..\\\\baichuan.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(data[:6])\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# VectorDB\n",
        "embedding = OpenAIEmbeddings(base_url=\"...\")\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kalin\\AppData\\Local\\Temp\\ipykernel_45592\\1791937861.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  base_docs = retriever.get_relevant_documents(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='evaluations, Baichuan 2 nearly doubles the results\\nof the Baichuan 1. In addition, Baichuan 2 also\\ndemonstrates strong performance on medical and\\nlegal domain tasks. On benchmarks such as\\nMedQA (Jin et al., 2021) and JEC-QA (Zhong\\net al., 2020), Baichuan 2 outperforms other open-\\nsource models, making it a suitable foundation\\nmodel for domain-specific optimization.\\nAdditionally, we also released two chat\\nmodels, Baichuan 2-7B-Chat and Baichuan 2-'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='With such a massive amount of training data,\\nBaichuan 2 achieves significant improvements over\\nBaichuan 1. On general benchmarks like MMLU\\n(Hendrycks et al., 2021a), CMMLU (Li et al.,\\n2023), and C-Eval (Huang et al., 2023), Baichuan\\n2-7B achieves nearly 30% higher performance\\ncompared to Baichuan 1-7B. Specifically, Baichuan\\n2 is optimized to improve performance on math\\nand code problems. On the GSM8K (Cobbe\\net al., 2021) and HumanEval (Chen et al., 2021)'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='Baichuan 1-13B-Base 52.40 51.60 55.30 49.69 43.20 43.01 26.76 11.59\\n13B\\nBaichuan 2-13B-Base 58.10 59.17 61.97 54.33 48.17 48.78 52.77 17.07\\nTable 1: Overall results of Baichuan 2 compared with other similarly sized LLMs on general benchmarks. * denotes\\nresults derived from official websites.\\nsystem. The composition of the training corpus is\\nshown in Figure 1.\\nFigure 1: The distribution of different categories of\\nBaichuan 2 training data.\\nData processing: For data processing, we focus'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='13B-Chat, optimized to follow human instructions.\\nThese models excel at dialogue and context\\nunderstanding. We will elaborate on our\\napproaches to improve the safety of Baichuan 2.\\nBy open-sourcing these models, we hope to enable\\nthe community to further improve the safety of\\nlarge language models, facilitating more research\\non responsible LLMs development.\\nFurthermore, in spirit of research collaboration\\n1https://commoncrawl.org/\\nand continuous improvement, we are also releasing'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='Baichuan 2: Open Large-scale Language Models\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\\nJian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\\nMang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='Baichuan 2 excels in vertical domains such\\nas medicine and law. We will release all\\npre-training model checkpoints to benefit the\\nresearch community in better understanding\\nthe training dynamics of Baichuan 2.\\n1 Introduction\\nThe field of large language models has witnessed\\npromising and remarkable progress in recent years.\\nThe size of language models has grown from\\nmillions of parameters, such as ELMo (Peters\\net al., 2018), GPT-1 (Radford et al., 2018), to'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='development and application of LLMs in specific\\nlanguages, such as Chinese.\\nIn this technical report, we introduce Baichuan\\n2, a series of large-scale multilingual language\\nmodels. Baichuan 2 has two separate models,\\nBaichuan 2-7B with 7 billion parameters and\\nBaichuan 2-13B with 13 billion parameters. Both\\nmodels were trained on 2.6 trillion tokens, which\\nto our knowledge is the largest to date, more than\\ndouble that of Baichuan 1 (Baichuan, 2023b,a).'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='2 Pre-training\\nThis section introduces the training procedure\\nfor the Baichuan 2 foundation models. Before\\ndiving into the model details, we first show the\\noverall performance of the Baichuan 2 base models\\ncompared to other open or closed-sourced models\\nin Table 1. We then describe our pre-training data\\nand data processing methods. Next, we elaborate\\non the Baichuan 2 architecture and scaling results.\\nFinally, we describe the distributed training system.\\n2.1 Pre-training Data'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 3, 'page_label': '4', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='(Taylor et al., 2022). Table 2 shows a detailed\\ncomparison of Baichuan 2’s tokenizer with others.\\n2.3.1 Positional Embeddings\\nBuilding on Baichuan 1, we adopt Rotary\\nPositional Embedding (RoPE) (Su et al., 2021)\\nfor Baichuan 2-7B and ALiBi (Press et al.,\\n2021) for Baichuan 2-13B. ALiBi is a more\\nrecent positional encoding technique that has\\nshown improved extrapolation performance.\\nHowever, most open-sourced models use RoPE for\\npositional embeddings, and optimized attention'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='feature engineering. However, most powerful\\nLLMs are closed-source or limited in their\\ncapability for languages other than English. In\\nthis technical report, we present Baichuan 2,\\na series of large-scale multilingual language\\nmodels containing 7 billion and 13 billion\\nparameters, trained from scratch, on 2.6 trillion\\ntokens. Baichuan 2 matches or outperforms\\nother open-source models of similar size on\\npublic benchmarks like MMLU, CMMLU,\\nGSM8K, and HumanEval. Furthermore,')]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_docs = retriever.get_relevant_documents(\n",
        "    \"What is baichuan2 ？\"\n",
        ")\n",
        "\n",
        "base_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_transformers import (\n",
        "    LongContextReorder,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "reordering = LongContextReorder()\n",
        "reordered_docs = reordering.transform_documents(base_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='With such a massive amount of training data,\\nBaichuan 2 achieves significant improvements over\\nBaichuan 1. On general benchmarks like MMLU\\n(Hendrycks et al., 2021a), CMMLU (Li et al.,\\n2023), and C-Eval (Huang et al., 2023), Baichuan\\n2-7B achieves nearly 30% higher performance\\ncompared to Baichuan 1-7B. Specifically, Baichuan\\n2 is optimized to improve performance on math\\nand code problems. On the GSM8K (Cobbe\\net al., 2021) and HumanEval (Chen et al., 2021)'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='13B-Chat, optimized to follow human instructions.\\nThese models excel at dialogue and context\\nunderstanding. We will elaborate on our\\napproaches to improve the safety of Baichuan 2.\\nBy open-sourcing these models, we hope to enable\\nthe community to further improve the safety of\\nlarge language models, facilitating more research\\non responsible LLMs development.\\nFurthermore, in spirit of research collaboration\\n1https://commoncrawl.org/\\nand continuous improvement, we are also releasing'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='Baichuan 2 excels in vertical domains such\\nas medicine and law. We will release all\\npre-training model checkpoints to benefit the\\nresearch community in better understanding\\nthe training dynamics of Baichuan 2.\\n1 Introduction\\nThe field of large language models has witnessed\\npromising and remarkable progress in recent years.\\nThe size of language models has grown from\\nmillions of parameters, such as ELMo (Peters\\net al., 2018), GPT-1 (Radford et al., 2018), to'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='2 Pre-training\\nThis section introduces the training procedure\\nfor the Baichuan 2 foundation models. Before\\ndiving into the model details, we first show the\\noverall performance of the Baichuan 2 base models\\ncompared to other open or closed-sourced models\\nin Table 1. We then describe our pre-training data\\nand data processing methods. Next, we elaborate\\non the Baichuan 2 architecture and scaling results.\\nFinally, we describe the distributed training system.\\n2.1 Pre-training Data'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='feature engineering. However, most powerful\\nLLMs are closed-source or limited in their\\ncapability for languages other than English. In\\nthis technical report, we present Baichuan 2,\\na series of large-scale multilingual language\\nmodels containing 7 billion and 13 billion\\nparameters, trained from scratch, on 2.6 trillion\\ntokens. Baichuan 2 matches or outperforms\\nother open-source models of similar size on\\npublic benchmarks like MMLU, CMMLU,\\nGSM8K, and HumanEval. Furthermore,'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 3, 'page_label': '4', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='(Taylor et al., 2022). Table 2 shows a detailed\\ncomparison of Baichuan 2’s tokenizer with others.\\n2.3.1 Positional Embeddings\\nBuilding on Baichuan 1, we adopt Rotary\\nPositional Embedding (RoPE) (Su et al., 2021)\\nfor Baichuan 2-7B and ALiBi (Press et al.,\\n2021) for Baichuan 2-13B. ALiBi is a more\\nrecent positional encoding technique that has\\nshown improved extrapolation performance.\\nHowever, most open-sourced models use RoPE for\\npositional embeddings, and optimized attention'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='development and application of LLMs in specific\\nlanguages, such as Chinese.\\nIn this technical report, we introduce Baichuan\\n2, a series of large-scale multilingual language\\nmodels. Baichuan 2 has two separate models,\\nBaichuan 2-7B with 7 billion parameters and\\nBaichuan 2-13B with 13 billion parameters. Both\\nmodels were trained on 2.6 trillion tokens, which\\nto our knowledge is the largest to date, more than\\ndouble that of Baichuan 1 (Baichuan, 2023b,a).'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='Baichuan 2: Open Large-scale Language Models\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\\nJian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\\nMang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='Baichuan 1-13B-Base 52.40 51.60 55.30 49.69 43.20 43.01 26.76 11.59\\n13B\\nBaichuan 2-13B-Base 58.10 59.17 61.97 54.33 48.17 48.78 52.77 17.07\\nTable 1: Overall results of Baichuan 2 compared with other similarly sized LLMs on general benchmarks. * denotes\\nresults derived from official websites.\\nsystem. The composition of the training corpus is\\nshown in Figure 1.\\nFigure 1: The distribution of different categories of\\nBaichuan 2 training data.\\nData processing: For data processing, we focus'),\n",
              " Document(metadata={'author': '', 'creationdate': '2025-04-18T00:32:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-04-18T00:32:55+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\..\\\\baichuan.pdf', 'subject': '', 'title': '', 'total_pages': 28, 'trapped': '/False'}, page_content='evaluations, Baichuan 2 nearly doubles the results\\nof the Baichuan 1. In addition, Baichuan 2 also\\ndemonstrates strong performance on medical and\\nlegal domain tasks. On benchmarks such as\\nMedQA (Jin et al., 2021) and JEC-QA (Zhong\\net al., 2020), Baichuan 2 outperforms other open-\\nsource models, making it a suitable foundation\\nmodel for domain-specific optimization.\\nAdditionally, we also released two chat\\nmodels, Baichuan 2-7B-Chat and Baichuan 2-')]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reordered_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
